{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deujahritik/12194824/blob/main/Week_13_Lab_GroupF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BLhFgvX4vs1L"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwoZfryIvs1M",
        "outputId": "984250b6-2eb1-4714-fd91-631596894e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enTwZolYvs1N",
        "outputId": "7bfb7301-1fe6-4fb6-f802-d0e165a9fb14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DME8NiLYvs1N"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FNjr-joOvs1N"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wsQEfI1yvs1O"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7JzaiGVvs1O",
        "outputId": "c121f728-c402-4873-f5f1-8c160b3e4315"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm5yT2uwvs1O",
        "outputId": "4e3e01d6-c211-4f9e-ef96-3eb394ae7f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fyyyst6PNDp",
        "outputId": "fafefe78-3cd2-42e4-abc2-28cba9d60c5a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmDq53zCPP64",
        "outputId": "80966ec3-e30f-486a-8f9a-094075d785fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  46.3M      0  0:00:01  0:00:01 --:--:-- 46.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "ioQhOsxDPVg0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WUG0x2RPaQZ",
        "outputId": "6ddc73c5-4eaf-4d92-f4a8-0351582cb99f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "metadata": {
        "id": "ud0o6Q_TPbwX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WMMGL7XPdXu",
        "outputId": "6c24d065-e842-4955-815c-89440bb11535"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaX0DdWlPfro",
        "outputId": "bbcbd35d-21cf-498e-d5df-8460d36f9a82"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'Why were there so many people crowding into an evening showing of Roberto Moreira\\'s \"Up Against Them All\" (\"Contra Todos\") at the San Francisco Film Festival? \"It\\'s about a hit man,\" my friend said. \"Well. . . and it\\'s Brazilian,\" I added. Beautiful multicolored people, tropical weather, lush rhythms, and a hip gangster plot? Ample enticements no doubt.<br /><br />Somebody forgot to tell us one little detail: this is a very bad movie, really pretty horrible, and as unpleasant to watch as it is poorly made.<br /><br />So how on earth did \"Contra Todos\" get to make the rounds of Berlin, Melbourne, London, Manila, Stockholm, Cairo, Chicago, numerous smaller local festivals, and now San Francisco? Apparently, because of the way the promotional process and the film festival circuit work.<br /><br />First of all, it won first prize at the Rio Film Festival where it was called the best Brazilian movie of the year. It must have been a bad year; they\\'ve had much, much better ones. Next, snappy synopses in catalogs plus imaginary buzz lead to crowded auditoriums and -- since the movie isn\\'t featured anywhere and so avoids close scrutiny by critics -- it keeps going the rounds.<br /><br />Festival blurbs aimed at promotion sometimes goose it up a lot. A Chicago Festival one called \"Contra Todos\" \"a speedball cocktail shot straight out of Brazil\" and referred to Claudia\\'s s boyfriend as the \"stud of the slum-like neighborhood.\" Soninha is \"Teodoro\\'s nymph-like teen-aged daughter of burgeoning sexuality.\" The movie is \"shot with the urgency of a frequently hand-held camera\" and the director \"works up a genuine and palpable sense of frustration borne from domestic desperation and decay.\" The effect is \" unbearably raw and honest,\" and the movie hurtles \"toward a conclusion as dead-ended as the lives on display.\" Not the best writing, but it sure pumps up the excitement for a certain kind of potential viewer.<br /><br />\"Contra Todos\" does concern a hit man, two hit men actually, and a wife and daughter and a born-again Christian girlfriend. It\\'s shot -- in execrably ugly digital video with no talent behind the camera-work -- mostly in a barren-looking poor suburb rather than in one of the teeming \"favelas\" or village-like Brazilian city slums where such wonderful films as \"Black Orpheus\", \"Pixote,\" and \"City of God\" were made, and not in Rio this time, but S\\xc3\\xa3o Paulo.<br /><br />The hit man with family problems is Teodoro ( Giulio Lopez) and his partner with a drug problem is Waldomiro (Ailtan Gra\\xc3\\xa7a). Both actors have a little TV experience as does the actress who plays Teodoro\\'s sluttish blonde wife Cl\\xc3\\xa1udia (Leona Cavalli) and Silvia Loren\\xc3\\xa7o who plays his pouting, ready-to-revolt daughter Soninha. These actors might make it through the back corners of a few telenovelas. Who knows? -- in a better directed film they might even be good. Aside from them there are some young men who get bumped off by Teordoro or, when he\\'s busy, gangs of thugs. The principals don\\'t work up much presence, even though the camera magnifies their pores.<br /><br />A couple of observers, one at the Berlin Festival and one at London\\'s, did see this movie\\'s failings but alas they\\'re buried in the Web hinterlands. Henry Sheehan noted from Berlin that the \"film\" (his quotes) was \"the worst of the video works\" shown. \"The filmmaker seems to have chosen video simply because it was a cheap alternative to film,\" Sheehan wrote, \"and hasn\\'t made any creative use of the new medium\" -- nor, he adds, done anything else creative.<br /><br />Sheehan pointed out the movie\\'s first big mistake: it \"starts off as a domestic drama that\\'s supposed to ratchet up when, half an hour into the action, Moreira reveals that the father and one of his friends are professional hit men. Waiting the thirty minutes adds nothing to the movie; it seems like a perfectly arbitrary decision and is, at the very least, a waste of time. But ratcheting up is all Moreira ever does, like a little kid who\\'s gotten a tool kit for his birthday, and goes around banging everything in sight without rhyme, reason or skill.\" Devastating, but true.<br /><br />Writing about the 2004 London festival for Kamera.com, Metin Alsanjak tried to look at the positive side but nonetheless gave away the lack of redeeming features in calling the performances \"easily the film\\'s best feature.\" Yes, very easily, given that everything else is so bad. Alsanjak admitted that \" this low-budget, violent and seedy account of the lawless in Sao Paulo is devoid of any likable characters, and as a result, of hope. Too dark and cynical to be a telling account of the human condition, the film is not helped by poor subtitling.. .\" Alsanjak\\'s connecting Contra Todos to \"Dogme\" and Mike Leigh didn\\'t help matters.<br /><br />Apart from that meaningless first half hour in which nothing redeems the boredom of our wait for the first acts of violence -- which, when they come, are just \"banging everything in sight without rhyme, reason or skill\" -- Moreira clumsily tries to redeem his abrupt finale by adding what appear to be outtakes right after it, followed by an implausible ironic concluding scene where one of the characters gets married. No doubt the director wanted to exhibit the \"banality of evil\" of low-level hit men in working class neighborhoods, but he can\\'t make the characters, which he sees generically, come alive for us. And the structure of the film shows that he also can\\'t edit his material. <br /><br />(Seen at the San Francisco International Film Festival on April 28, 2005.)', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "_iVau7bkPhVY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOZxbmFrPjeQ",
        "outputId": "15beef94-94a8-4dc7-aa66-6bfbb5c54ae6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "fNn62iz2Prf6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9nxaU0NPyWM",
        "outputId": "ba4f0c1c-289c-4b14-d71f-91e2d2bfaf80"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 15s 23ms/step - loss: 0.3999 - accuracy: 0.8346 - val_loss: 0.2862 - val_accuracy: 0.8908\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2705 - accuracy: 0.9025 - val_loss: 0.2831 - val_accuracy: 0.8902\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2369 - accuracy: 0.9189 - val_loss: 0.2980 - val_accuracy: 0.8934\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2257 - accuracy: 0.9241 - val_loss: 0.3179 - val_accuracy: 0.8878\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2177 - accuracy: 0.9292 - val_loss: 0.3282 - val_accuracy: 0.8904\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2090 - accuracy: 0.9330 - val_loss: 0.3311 - val_accuracy: 0.8886\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2148 - accuracy: 0.9334 - val_loss: 0.3452 - val_accuracy: 0.8820\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2052 - accuracy: 0.9340 - val_loss: 0.3701 - val_accuracy: 0.8798\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2035 - accuracy: 0.9354 - val_loss: 0.3663 - val_accuracy: 0.8828\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2038 - accuracy: 0.9358 - val_loss: 0.3632 - val_accuracy: 0.8874\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.2871 - accuracy: 0.8890\n",
            "Test acc: 0.889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ],
      "metadata": {
        "id": "xbM1NHm0P0VJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMRQ1qYPP3XZ",
        "outputId": "80412fcf-fa61-4704-f3f6-9672a5c9841c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 21ms/step - loss: 0.3756 - accuracy: 0.8472 - val_loss: 0.2622 - val_accuracy: 0.8982\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2403 - accuracy: 0.9152 - val_loss: 0.2687 - val_accuracy: 0.9040\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2110 - accuracy: 0.9323 - val_loss: 0.2991 - val_accuracy: 0.8974\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1965 - accuracy: 0.9381 - val_loss: 0.3132 - val_accuracy: 0.9004\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.1885 - accuracy: 0.9445 - val_loss: 0.3239 - val_accuracy: 0.9022\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1924 - accuracy: 0.9451 - val_loss: 0.3452 - val_accuracy: 0.8976\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.1709 - accuracy: 0.9474 - val_loss: 0.3629 - val_accuracy: 0.8962\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1746 - accuracy: 0.9510 - val_loss: 0.3622 - val_accuracy: 0.8972\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.1716 - accuracy: 0.9528 - val_loss: 0.3652 - val_accuracy: 0.8980\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.1728 - accuracy: 0.9535 - val_loss: 0.3741 - val_accuracy: 0.9002\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.2638 - accuracy: 0.8986\n",
            "Test acc: 0.899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ],
      "metadata": {
        "id": "TZGahdc4P5ED"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ],
      "metadata": {
        "id": "0KafKxY4P7Ak"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BobYSZvP8gD",
        "outputId": "dd068e71-7dea-48c8-a09c-407e51d946f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 14s 22ms/step - loss: 0.4822 - accuracy: 0.7941 - val_loss: 0.2792 - val_accuracy: 0.8958\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2985 - accuracy: 0.8858 - val_loss: 0.3040 - val_accuracy: 0.8906\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2554 - accuracy: 0.9025 - val_loss: 0.3096 - val_accuracy: 0.9026\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2352 - accuracy: 0.9108 - val_loss: 0.3255 - val_accuracy: 0.8986\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 5s 9ms/step - loss: 0.2256 - accuracy: 0.9127 - val_loss: 0.3347 - val_accuracy: 0.8912\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2185 - accuracy: 0.9153 - val_loss: 0.3409 - val_accuracy: 0.8954\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2115 - accuracy: 0.9145 - val_loss: 0.3463 - val_accuracy: 0.8904\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2104 - accuracy: 0.9147 - val_loss: 0.3496 - val_accuracy: 0.8910\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2049 - accuracy: 0.9143 - val_loss: 0.3665 - val_accuracy: 0.8856\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2019 - accuracy: 0.9177 - val_loss: 0.3670 - val_accuracy: 0.8894\n",
            "782/782 [==============================] - 11s 13ms/step - loss: 0.2870 - accuracy: 0.8926\n",
            "Test acc: 0.893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "FE1y2SykP99e"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcaMk8gpQ1Mp",
        "outputId": "503f445f-69c0-40bf-aaa3-1e201b8210ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87.38 percent positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fKUeL-S1Q-Qq"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}